

# ComputerVisionProject

Complete pipeline for multi-camera 3D reconstruction of moving objects using YOLO, tracking, interpolation, triangulation, and visualization.

## Description
This project allows you to:
- Run YOLO inference on multi-camera videos
- Perform 2D tracking and temporal interpolation of detections
- Triangulate 3D points from multiple views
- Evaluate performance against ground truth
- Visualize results in 3D

## Requirements
- Python 3.8+
- [Ultralytics](https://docs.ultralytics.com/) (`pip install ultralytics`)
- OpenCV, NumPy, Matplotlib, PyYAML, torch
- Recommended: Conda environment

## Installation

### Option 1: Conda (Recommended)
Create the environment directly from the `environment.yml` file. This ensures all dependencies (including system libraries) are correct.

```bash
conda env create -f environment.yml
conda activate cv
```

### Option 2: Pip
If you prefer using pip or a standard virtual environment:

```bash
# Create a virtual environment (optional but recommended)
python -m venv venv
# Windows
.\venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

## Folder Structure

```
dataset.yaml
fine_tune.py
pipeline.py
README.md
run.sh
src/
	config.py
	evaluation.py
	interpoler.py
	metrics_3d.py
	rectified_videos.py
	tracking_2d.py
	triangulation_3d.py
	visualizer_3d.py
	__init__.py
camparams/
	out13_camera_calib.json
	out13_img_points.json
	...
dataset/
	val/
		images/
		labels/
			...
	video/
runs/
	tracks/
	trajectories/
	triangulation/
		points.csv
		points.json
weights/
	best.pt
```

- `src/` : core python modules for the pipeline
- `dataset/` : images, YOLO labels, videos, split into train/val/test, and subfolders for inference and video
- `camparams/` : camera calibration files (JSON)
- `weights/` : pre-trained and fine-tuned YOLO models
- `runs/` : results for inference, triangulation, evaluation, and more
- Main scripts: `pipeline.py`, `fine_tune.py`

## Main Pipeline
The complete pipeline is managed by `pipeline.py` and can be configured via `run.sh` or command line arguments.

### Typical Flow:
1. YOLO inference on multi-camera videos (`--infer-videos`)
2. Detection interpolation (optional, `--interpolate`)
3. Optical distortion rectification (optional, `--rectify`)
4. Multi-camera 3D triangulation
5. Evaluation against ground truth (optional, `--evaluate-labels`)
6. Interactive 3D visualization (optional, `--visualize`)

## How to Run the Pipeline

### Full execution (from PowerShell or bash):
```bash
sh run.sh
# or manually:
python pipeline.py --infer-videos --interpolate --rectify --evaluate-labels --visualize --device 0
```

### Main options (`pipeline.py`):
- `--cameras 2 4 13` : camera IDs to process
- `--labels-dir dataset/infer_video` : labels directory
- `--infer-videos` : run YOLO inference on videos
- `--interpolate` : enable detection interpolation
- `--rectify` : correct optical distortions
- `--evaluate-labels` : evaluate IoU against ground truth
- `--visualize` : visualize 3D results
- `--model weights/fine_tuned_yolo_final.pt` : YOLO model to use
- `--device 0` : GPU/CPU

For all options see `src/config.py` or run `python pipeline.py --help`.

## Custom YOLO Training
To train/fine-tune YOLO on a custom dataset:
```bash
python fine_tune.py
# Edit parameters/dataset in fine_tune.py if needed
```
The trained model will be saved in `weights/`.

## Evaluation
To evaluate the pipeline (IoU metrics) against ground truth:
```bash
python pipeline.py --labels-dir dataset/infer_video --evaluate-labels --eval-gt-dir dataset/val/labels
```

## 3D Triangulation and Visualization
To triangulate 3D points from YOLO labels and visualize them:
```bash
python pipeline.py --labels-dir dataset/infer_video --visualize
```

## Dataset
The expected structure is in `dataset/`:
- `train/`, `val/`, `test/` with subfolders `images/` and `labels/` (YOLO format)
- `video/` for original videos
- `infer_video/` for labels generated by inference

## Credits
Developed by [your name/team].
Based on Ultralytics YOLOv8.
All code and documentation are provided in English for clarity and collaboration.
